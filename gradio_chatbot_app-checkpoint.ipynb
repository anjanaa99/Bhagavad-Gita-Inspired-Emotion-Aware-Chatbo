{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyqHXpfzNTi5",
    "outputId": "9612f9b9-1b51-4469-e50c-d0a0ffdc8e9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fschat 0.2.23 requires accelerate>=0.21, but you have accelerate 0.20.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio==3.41.0 transformers==4.32.0 langchain==0.0.273 -Uqqq\n",
    "!pip install accelerate==0.20.3 bitsandbytes==0.41.1 einops==0.7.0 peft==0.5.0 -Uqqq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Bishal\\AppData\\Local\\Programs\\Python\\Python310\\Lib\\site-packages\\~-kenizers'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "parlai 1.7.2 requires tensorboardX<=2.5.0, which is not installed.\n",
      "parlai 1.7.2 requires attrs~=20.2.0, but you have attrs 23.2.0 which is incompatible.\n",
      "parlai 1.7.2 requires fsspec~=2022.2.0, but you have fsspec 2024.5.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.32.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 41.0/43.8 kB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------------- 43.8/43.8 kB 74.1 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.8.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bishal\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.1 MB 2.3 MB/s eta 0:00:04\n",
      "    --------------------------------------- 0.2/9.1 MB 2.5 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/9.1 MB 4.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.1 MB 4.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.3/9.1 MB 4.9 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.5/9.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 1.7/9.1 MB 4.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 1.9/9.1 MB 4.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.1/9.1 MB 4.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.2/9.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.4/9.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 2.6/9.1 MB 4.4 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.8/9.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.9/9.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 3.1/9.1 MB 4.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 3.3/9.1 MB 4.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/9.1 MB 4.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.7/9.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.8/9.1 MB 4.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.0/9.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.2/9.1 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.4/9.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.6/9.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.4/9.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.6/9.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.8/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.1/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/9.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.9/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.6/9.1 MB 3.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.9/9.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.8/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.2/2.2 MB 37.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.4/2.2 MB 17.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.9/2.2 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.2/2.2 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 1.9 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.0\n",
      "    Uninstalling transformers-4.32.0:\n",
      "      Successfully uninstalled transformers-4.32.0\n",
      "Successfully installed tokenizers-0.19.1 transformers-4.41.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7uflzy2_OAjL"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import re, os, warnings\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, GenerationConfig\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OYx4Jyh2OV17"
   },
   "outputs": [],
   "source": [
    "# initialize and load PEFT model and tokenizer\n",
    "def init_model_and_tokenizer(PEFT_MODEL):\n",
    "  config = PeftConfig.from_pretrained(PEFT_MODEL)\n",
    "  bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    force_download=True\n",
    "  )\n",
    "\n",
    "  peft_base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    return_dict=True,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "  )\n",
    "\n",
    "  peft_model = PeftModel.from_pretrained(peft_base_model, PEFT_MODEL)\n",
    "\n",
    "  peft_tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "  peft_tokenizer.pad_token = peft_tokenizer.eos_token\n",
    "\n",
    "  return peft_model, peft_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Azg2zEW2OkP7"
   },
   "outputs": [],
   "source": [
    "# custom LLM chain to generate answer from PEFT model for each query\n",
    "def init_llm_chain(peft_model, peft_tokenizer):\n",
    "    class CustomLLM(LLM):\n",
    "        def _call(self, prompt: str, stop=None, run_manager=None) -> str:\n",
    "            device = \"cuda:0\"\n",
    "            peft_encoding = peft_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "            peft_outputs = peft_model.generate(input_ids=peft_encoding.input_ids, generation_config=GenerationConfig(max_new_tokens=256, pad_token_id = peft_tokenizer.eos_token_id, \\\n",
    "                                                                                                                     eos_token_id = peft_tokenizer.eos_token_id, attention_mask = peft_encoding.attention_mask, \\\n",
    "                                                                                                                     temperature=0.4, top_p=0.6, repetition_penalty=1.3, num_return_sequences=1,))\n",
    "            peft_text_output = peft_tokenizer.decode(peft_outputs[0], skip_special_tokens=True)\n",
    "            return peft_text_output\n",
    "\n",
    "        @property\n",
    "        def _llm_type(self) -> str:\n",
    "            return \"custom\"\n",
    "\n",
    "    llm = CustomLLM()\n",
    "\n",
    "    template = \"\"\"Answer the following question truthfully.\n",
    "    If you don't know the answer, respond 'Sorry, I don't know the answer to this question.'.\n",
    "    If the question is too complex, respond 'Kindly, consult a psychiatrist for further queries.'.\n",
    "\n",
    "    Example Format:\n",
    "    <HUMAN>: question here\n",
    "    <ASSISTANT>: answer here\n",
    "\n",
    "    Begin!\n",
    "\n",
    "    <HUMAN>: {query}\n",
    "    <ASSISTANT>:\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"query\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    return llm_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "10h_KVGilk2J"
   },
   "outputs": [],
   "source": [
    "def user(user_message, history):\n",
    "  return \"\", history + [[user_message, None]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "QeFE1qZnluMm"
   },
   "outputs": [],
   "source": [
    "def bot(history):\n",
    "  if len(history) >= 2:\n",
    "    query = history[-2][0] + \"\\n\" + history[-2][1] + \"\\nHere, is the next QUESTION: \" + history[-1][0]\n",
    "  else:\n",
    "    query = history[-1][0]\n",
    "\n",
    "  bot_message = llm_chain.run(query)\n",
    "  bot_message = post_process_chat(bot_message)\n",
    "\n",
    "  history[-1][1] = \"\"\n",
    "  history[-1][1] += bot_message\n",
    "  return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aae3uAD5lyXN"
   },
   "outputs": [],
   "source": [
    "def post_process_chat(bot_message):\n",
    "  try:\n",
    "    bot_message = re.findall(r\"<ASSISTANT>:.*?Begin!\", bot_message, re.DOTALL)[1]\n",
    "  except IndexError:\n",
    "    pass\n",
    "\n",
    "  bot_message = re.split(r'<ASSISTANT>\\:?\\s?', bot_message)[-1].split(\"Begin!\")[0]\n",
    "\n",
    "  bot_message = re.sub(r\"^(.*?\\.)(?=\\n|$)\", r\"\\1\", bot_message, flags=re.DOTALL)\n",
    "  try:\n",
    "    bot_message = re.search(r\"(.*\\.)\", bot_message, re.DOTALL).group(1)\n",
    "  except AttributeError:\n",
    "    pass\n",
    "\n",
    "  bot_message = re.sub(r\"\\n\\d.$\", \"\", bot_message)\n",
    "  bot_message = re.split(r\"(Goodbye|Take care|Best Wishes)\", bot_message, flags=re.IGNORECASE)[0].strip()\n",
    "  bot_message = bot_message.replace(\"\\n\\n\", \"\\n\")\n",
    "\n",
    "  return bot_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "34b559e7dcc245d59cf63059f89854e1",
      "c6e4d63554564591ab38a4e633f60ba4",
      "822036e3276b40a98c989dd7af3b690d",
      "e9c50bc4ec04407386d5a4dea42a18bd",
      "350fa3821e104536b42c1f70985f1ee4",
      "075c92e6397242e495834f5dbdd074fe",
      "06c033e936fb4b9faaa2a13150855a04",
      "3306aab2825e424db73536be38fe774e",
      "9c6feacfae344692b5c71d16d687c74e",
      "20c8b95e503a44de81905b652b9291c7",
      "b05c18c3366a48bf81c1260e553995ff"
     ]
    },
    "id": "cjZ9ENNnSpeY",
    "outputId": "671b81ea-4789-4555-e96a-c69797cb6a13"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lm_head.weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheliosbrahma/falcon-7b-sharded-bf16-finetuned-mental-health-conversational\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Add force_download=True to the function call\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m peft_model, peft_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43minit_model_and_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPEFT_MODEL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 12\u001b[0m, in \u001b[0;36minit_model_and_tokenizer\u001b[1;34m(PEFT_MODEL)\u001b[0m\n\u001b[0;32m      3\u001b[0m config \u001b[38;5;241m=\u001b[39m PeftConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(PEFT_MODEL)\n\u001b[0;32m      4\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      5\u001b[0m   load_in_4bit\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m   bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m   force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     10\u001b[0m )\n\u001b[1;32m---> 12\u001b[0m peft_base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m peft_model \u001b[38;5;241m=\u001b[39m PeftModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(peft_base_model, PEFT_MODEL)\n\u001b[0;32m     22\u001b[0m peft_tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbase_model_name_or_path)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    515\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    518\u001b[0m     )\n\u001b[0;32m    519\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    520\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    522\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3091\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3081\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3082\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[0;32m   3084\u001b[0m     (\n\u001b[0;32m   3085\u001b[0m         model,\n\u001b[0;32m   3086\u001b[0m         missing_keys,\n\u001b[0;32m   3087\u001b[0m         unexpected_keys,\n\u001b[0;32m   3088\u001b[0m         mismatched_keys,\n\u001b[0;32m   3089\u001b[0m         offload_index,\n\u001b[0;32m   3090\u001b[0m         error_msgs,\n\u001b[1;32m-> 3091\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3092\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3094\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[0;32m   3095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3098\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3099\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3102\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3103\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_quantized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantization_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mQuantizationMethod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBITS_AND_BYTES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3109\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_4bit \u001b[38;5;241m=\u001b[39m load_in_4bit\n\u001b[0;32m   3110\u001b[0m model\u001b[38;5;241m.\u001b[39mis_loaded_in_8bit \u001b[38;5;241m=\u001b[39m load_in_8bit\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3408\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[0;32m   3406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3407\u001b[0m         weight_map \u001b[38;5;241m=\u001b[39m {p: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, f) \u001b[38;5;28;01mfor\u001b[39;00m p, f \u001b[38;5;129;01min\u001b[39;00m sharded_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m-> 3408\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3409\u001b[0m         p: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafetensors_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: str_dtype}\n\u001b[0;32m   3410\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p, f \u001b[38;5;129;01min\u001b[39;00m weight_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   3411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m param_device_map[p] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3412\u001b[0m     }\n\u001b[0;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3415\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[0;32m   3416\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   3417\u001b[0m         state_dict,\n\u001b[0;32m   3418\u001b[0m         model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3422\u001b[0m         ignore_mismatched_sizes,\n\u001b[0;32m   3423\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\modeling_utils.py:3411\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3406\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3407\u001b[0m         weight_map \u001b[38;5;241m=\u001b[39m {p: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, f) \u001b[38;5;28;01mfor\u001b[39;00m p, f \u001b[38;5;129;01min\u001b[39;00m sharded_metadata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m   3408\u001b[0m     offload_index \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3409\u001b[0m         p: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafetensors_file\u001b[39m\u001b[38;5;124m\"\u001b[39m: f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: p, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: str_dtype}\n\u001b[0;32m   3410\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p, f \u001b[38;5;129;01min\u001b[39;00m weight_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m-> 3411\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mparam_device_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisk\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3412\u001b[0m     }\n\u001b[0;32m   3414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3415\u001b[0m     \u001b[38;5;66;03m# Whole checkpoint\u001b[39;00m\n\u001b[0;32m   3416\u001b[0m     mismatched_keys \u001b[38;5;241m=\u001b[39m _find_mismatched_keys(\n\u001b[0;32m   3417\u001b[0m         state_dict,\n\u001b[0;32m   3418\u001b[0m         model_state_dict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3422\u001b[0m         ignore_mismatched_sizes,\n\u001b[0;32m   3423\u001b[0m     )\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lm_head.weight'"
     ]
    }
   ],
   "source": [
    "model = \"heliosbrahma/falcon-7b-sharded-bf16-finetuned-mental-health-conversational\"\n",
    "\n",
    "# Add force_download=True to the function call\n",
    "peft_model, peft_tokenizer = init_model_and_tokenizer(PEFT_MODEL=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7v4BVOyxQeik"
   },
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.HTML(\"\"\"<h1>Welcome to Mental Health Conversational AI</h1>\"\"\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"Chatbot specifically designed to provide psychoeducation, offer non-judgemental and empathetic support, self-assessment and monitoring.<br>\n",
    "        Get instant response for any mental health related queries. If the chatbot seems you need external support, then it will respond appropriately.<br>\"\"\"\n",
    "    )\n",
    "\n",
    "    chatbot = gr.Chatbot()\n",
    "    query = gr.Textbox(label=\"Type your query here, then press 'enter' and scroll up for response\")\n",
    "    clear = gr.Button(value=\"Clear Chat History!\")\n",
    "    clear.style(size=\"sm\")\n",
    "\n",
    "    llm_chain = init_llm_chain(peft_model, peft_tokenizer)\n",
    "\n",
    "    query.submit(user, [query, chatbot], [query, chatbot], queue=False).then(bot, chatbot, chatbot)\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue().launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bl3UYt3dUF6H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06c033e936fb4b9faaa2a13150855a04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "075c92e6397242e495834f5dbdd074fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20c8b95e503a44de81905b652b9291c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3306aab2825e424db73536be38fe774e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34b559e7dcc245d59cf63059f89854e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c6e4d63554564591ab38a4e633f60ba4",
       "IPY_MODEL_822036e3276b40a98c989dd7af3b690d",
       "IPY_MODEL_e9c50bc4ec04407386d5a4dea42a18bd"
      ],
      "layout": "IPY_MODEL_350fa3821e104536b42c1f70985f1ee4"
     }
    },
    "350fa3821e104536b42c1f70985f1ee4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "822036e3276b40a98c989dd7af3b690d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3306aab2825e424db73536be38fe774e",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c6feacfae344692b5c71d16d687c74e",
      "value": 8
     }
    },
    "9c6feacfae344692b5c71d16d687c74e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b05c18c3366a48bf81c1260e553995ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c6e4d63554564591ab38a4e633f60ba4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_075c92e6397242e495834f5dbdd074fe",
      "placeholder": "​",
      "style": "IPY_MODEL_06c033e936fb4b9faaa2a13150855a04",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "e9c50bc4ec04407386d5a4dea42a18bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_20c8b95e503a44de81905b652b9291c7",
      "placeholder": "​",
      "style": "IPY_MODEL_b05c18c3366a48bf81c1260e553995ff",
      "value": " 8/8 [01:32&lt;00:00,  9.87s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
